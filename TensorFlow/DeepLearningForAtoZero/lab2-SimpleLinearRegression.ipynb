{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|    2.4520|    0.3760| 45.660004\n",
      "   10|    1.1036|    0.0034|  0.206336\n",
      "   20|    1.0128|   -0.0209|  0.001026\n",
      "   30|    1.0065|   -0.0218|  0.000093\n",
      "   40|    1.0059|   -0.0212|  0.000083\n",
      "   50|    1.0057|   -0.0205|  0.000077\n",
      "   60|    1.0055|   -0.0198|  0.000072\n",
      "   70|    1.0053|   -0.0192|  0.000067\n",
      "   80|    1.0051|   -0.0185|  0.000063\n",
      "   90|    1.0050|   -0.0179|  0.000059\n",
      "  100|    1.0048|   -0.0173|  0.000055\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Data\n",
    "x_data = [1,2, 3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "\n",
    "# W, b initialize\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(100+1):  # W, b update\n",
    "    # Gradient descent(경사 하강 알고리즘) : 변수(W,b)에 자신의 미분값*learning_rate을 빼며 cost를 구한다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W * x_data + b\n",
    "        # reduce_mean : 차원(rank)가 1(리스트)에서 0(값)으로 줄면서(reduce) 평균(mean)을 구한다.\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    # A.assign_sub(B) : A -= B\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    if i % 10 == 0:\n",
    "        print(\"{:5}|{:10.4f}|{:10.4f}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent (경사 하강 알고리즘)\n",
    "경사 하강법(傾斜下降法, Gradient descent)은 1차 근삿값 발견용 최적화 알고리즘이다. 기본 개념은 함수의 기울기(경사)를 구하고 경사의 절댓값이 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것이다.\n",
    "\n",
    "최적화할 함수 f(x)에 대하여, 먼저 시작점 x0를 정한다. 현재 xi가 주어졌을 때, 그 다음으로 이동할 점인 x(i+1)은 다음과 같이 계산된다.\n",
    "\n",
    "x(i+1) = xi - a*(f(xi)의 미분값)\n",
    "이때 a는 이동할 거리를 조절하는 매개변수(learning_rate)이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
