{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "$$ S(y_i) = \\frac{e^{y_i}}{\\sum_{i} e^{y_i}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function : cross entropy\n",
    "$S$ : Softmax Regression 함수\n",
    "\n",
    "$L$ : lable(실제 값)\n",
    "\n",
    "$$ D(S,L) = -{\\sum_{i} L_i\\log{S_i}} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, Loss: 2.7990\n",
      "iter: 300, Loss: 0.8312\n",
      "iter: 600, Loss: 0.6635\n",
      "iter: 900, Loss: 0.6005\n",
      "iter: 1200, Loss: 0.5601\n",
      "iter: 1500, Loss: 0.5296\n",
      "iter: 1800, Loss: 0.5050\n",
      "iter: 2100, Loss: 0.4844\n",
      "iter: 2400, Loss: 0.4667\n",
      "iter: 2700, Loss: 0.4510\n",
      "iter: 3000, Loss: 0.4369\n",
      "tf.Tensor(\n",
      "[[9.9414093e-03 5.8100652e-02 9.3195790e-01]\n",
      " [1.6830681e-02 2.0065969e-01 7.8250962e-01]\n",
      " [4.1880864e-03 4.3612772e-01 5.5968416e-01]\n",
      " [3.4023379e-03 5.9760123e-01 3.9899644e-01]\n",
      " [5.5367953e-01 3.9984158e-01 4.6478894e-02]\n",
      " [2.7209947e-01 7.2752315e-01 3.7744248e-04]\n",
      " [6.0211343e-01 3.9546511e-01 2.4214946e-03]\n",
      " [7.0973784e-01 2.8983405e-01 4.2811382e-04]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([2 2 2 1 0 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    " [2, 1, 3, 2],\n",
    " [3, 1, 3, 4],\n",
    " [4, 1, 5, 5],\n",
    " [1, 7, 5, 5],\n",
    " [1, 2, 5, 6],\n",
    " [1, 6, 6, 6],\n",
    " [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    " [0, 0, 1],\n",
    " [0, 0, 1],\n",
    " [0, 1, 0],\n",
    " [0, 1, 0],\n",
    " [0, 1, 0],\n",
    " [1, 0, 0],\n",
    " [1, 0, 0]]\n",
    "\n",
    "# 3개의 클래스를 분류할 때 0,1,2를 각각 [1,0,0],[0,1,0],[0,0,1]로 하나만 hot하게 표시함\n",
    "# => one-hot encoding\n",
    "x_data = np.asarray(x_data, dtype = np.float32)\n",
    "y_data = np.asarray(y_data, dtype = np.float32)\n",
    "\n",
    "# y의 개수 = 클래스 개수 = label개수\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
    "W = tf.Variable(tf.random.normal([4,3]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([3]), name='bias')\n",
    "variable = [W,b]\n",
    "\n",
    "dataset.element_spec\n",
    "\n",
    "def softmax_fn(features):\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(features,W)+b)\n",
    "    return hypothesis\n",
    "\n",
    "def loss_fn(features, labels):\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(features,W)+b)\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y_data*tf.math.log(hypothesis),axis=1))\n",
    "    return cost\n",
    "\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(features, labels)\n",
    "    return tape.gradient(loss_value, [W, b])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "\n",
    "n_epochs = 3000\n",
    "for step in range(n_epochs + 1):\n",
    "    \n",
    "    for features, labels in iter(dataset):\n",
    "        hypothesis = softmax_fn(features)\n",
    "        grads = grad(hypothesis, features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars = zip(grads, [W, b]))\n",
    "    \n",
    "    if step % 300 == 0:\n",
    "            print(\"iter: {}, Loss: {:.4f}\".format(step, loss_fn(features, labels)))\n",
    "\n",
    "a = x_data\n",
    "a = softmax_fn (a)\n",
    "print(hypothesis) #softmax 함수를 통과시킨 x_data\n",
    "\n",
    "#argmax 가장큰 값의index를 찾아줌\n",
    "\n",
    "print(tf.argmax(a,1)) #가설을 통한 예측값\n",
    "print(tf.argmax(y_data,1)) #실제 값"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
